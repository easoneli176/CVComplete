---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# CVComplete

<!-- badges: start -->
<!-- badges: end -->

The goal of CVComplete is to package all the functions Lizzie has had to build on her own when working with cross validation. It harnesses functions from a myriad of other packages, and tailors their powers to perform CV and output the metrics Lizzie often needs for her work.

Cross validation is used to determine the effectiveness of a particular modeling method. In k-fold cross validation, rather than a single simple train-test split, we harness the power of the entire dataset to determine how the method performs on unseen data by iteratively applying the method to different subsets of the data and testing it on the unseen subset. The metrics calculated from k-fold cross validation are averaged across the k-folds and assumed to apply to the modeling method itself. Thus, once the predictive power of the method has been established through k-fold cross validation, typically we re-fit the final model using the method on the full dataset. There is some controversy over this refitting step as sometimes the size of a dataset can change predictive power, but this is the general practice.

Typically, data scientists are interested in calculating performance metrics on the out of fold predictions from cross validation and using them to determine model predictive power. However, Lizzie has found there is often more to consider than metrics such as the AIC, BIC, RMSE, R^2, etc. Particularly in the case of linear models, we can evaluate the stability of linear coefficients across models created from different segments of data. Further, we can consider how often models overfit the data across folds, or how often a certain coefficient was statistically significant. That's where this package comes into play. This package automates coefficient comparison across folds, and gives the user easy-to-work-with fields for folds and out of fold predictions, which allow the user to calculate their own creative metrics if they so desire.

Further, occasionally the method to be tested with cross validation goes beyond modeling alone, and feature engineering is also part of the cross validation method to be tested. For example, imputation methods often use the mean of the column, which should be re-calculated for each new fold combination. The function featengcv makes this process significantly easier.


## Installation

You can install the development version of CVComplete from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("easoneli176/CVComplete")
```

## Example

The first step to cross validation is designating the folds. The function created in this package appends a field to the data called "fold" that assigns each observation to a fold. This process can be done with stratification or without. For example:

```{r example}
library(CVComplete)

mock_data <- data.frame(preds = rep(1:4,10),target=rep(1:5,8),strat=rep(1:2,20))

#with stratification
newdat<-fold_field(mock_data,3,"strat")

#without stratification
newdat2<-fold_field(mock_data,3)

#check that stratification worked:
table(newdat$fold,newdat$strat)

#check that stratification did not occur:
table(newdat2$fold,newdat$strat)

#All functions from this point on can be given a dataset that has already been assigned folds, 
#or it will assign the folds themselves.

#Buildcvmods automates the building of cv models and outputs the models into a list. 
#This is handy if the user wants to evaluate each model in some way outside 
#this package's capabilities. 

#create mock dataframe
mock_data <- data.frame(preds1 = rep(1:4,10),preds2 = rep(1:10,4), 
                        target=rep(1:5,8),strat=rep(1:2,20),weights = rep(c(2,1),20))

#The user must create their own modeling function that intakes a dataset 
#and outputs the model built in the desired methodology. 
#While this is tedious, it is necessary to be able to work with a 
#myriad of modeling methodologies.

#model function:
model_function<-function(data){
 targnum<-which(colnames(data) == "target")
 weightnum<-which(colnames(data) == "weights")
 moddat<-data[,-c(targnum,weightnum)]
 targ<-data[,targnum]
 weights<-data[,weightnum]
 set.seed(123)
 mod<-lm(targ~.,data=moddat,weights=weights)
 mod
 }
#test function:
summary(model_function(mock_data))
#compare to:
mock_data2 <- mock_data[,-which(colnames(mock_data) == "weights")]
summary(lm(target~.,data=mock_data2,weights=mock_data$weights))

#finally, if those match, test functions:
mymods<-buildcvmods(mock_data,3,modelfunc=model_function)

datwpreds<-oofpreds(mock_data,3,modelfunc=model_function)

datwpreds

coeffdat<-coeffcomp(mock_data,3,modelfunc=model_function)

coeffdat

#Note that if the folds are predefined, we do not need the numfolds argument. 
#These objects should match the ones above since all of these functions call foldfield:

newdat2<-fold_field(mock_data,3)

mymods2<-buildcvmods(newdat2,modelfunc=model_function)

datwpreds2<-oofpreds(newdat2,modelfunc=model_function)

datwpreds2

coeffdat2<-coeffcomp(newdat2,modelfunc=model_function)

coeffdat2

#Finally, we showcase the feature engineering function, which is the most complex.

mock_data <- data.frame(col1 = as.numeric(c(rep(1:4,9), rep("NA",4))), 
                        col2 = as.numeric(c(rep("NA",4),rep(1:9,4))),strat=rep(c(1,2),20))

#The user must write their own engineering function which performs all necessary 
#feature engineering to be done on each CV run, and critically this function 
#must output the model that repeats that process on the out of fold data. 
#This is tricky to write, but it is needed to provide user flexibility, 
#and our example helps:

#engineer function:
eng_function<-function(data){
 col1<-data[,which(colnames(data) == "col1")]
 col2<-data[,which(colnames(data) == "col2")]
 val1<-mean(col1,na.rm=TRUE)
 val2<-median(col2,na.rm=TRUE)
 mod<-function(data){
 col1<-data[,which(colnames(data) == "col1")]
 col2<-data[,which(colnames(data) == "col2")]
 data[,which(colnames(data) == "col1")]<-ifelse(is.na(col1),val1,col1)
 data[,which(colnames(data) == "col2")]<-ifelse(is.na(col2),val2,col2)
 data
 }
 mod
}

#test function:
impfunc<-eng_function(mock_data)
impfunc(mock_data)

#Notice col2 had 5 imputed and col1 had 2.5. 
#Now, we create a new dataset where the names of the columns are the same,
#but the means/medians will be different.

mock_data2 <- data.frame(col2 = as.numeric(c(rep(1:4,9),rep("NA",4))),
                         col1 = as.numeric(c(rep("NA",4),rep(1:9,4))))

impfunc(mock_data2)

#Critically, col1 still had 2.5 imputed and col2 had 5 imputed 
#even though those are not the means/medians of those columns. 
#This indicates the function will work for out of fold data.

#Now, we test the real function

test1<-featengcv(mock_data,5,"strat",eng_function)
#We now test the imputation

testdat<-fold_field(mock_data,5,"strat")

#fold 5 has a missing value for col2, so it should be the median of col2 in folds 1-4:

median(testdat[!testdat$fold == 5,]$col2,na.rm=TRUE)

#Indeed, in test1, fold 5, we see the value 5 in what was originally row 2. It works!!!
#Now, we can perform regular modeling cross validation on this data,
#and the current fold structure will be maintained, so the full "method" we would
#be testing if we harnessed featengcv along with buildcvmods would be
#both the engineering and the modeling methodology.

```
