---
output: html_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# CVComplete

<!-- badges: start -->
<!-- badges: end -->

The goal of CVComplete is to package all the functions Lizzie has had to build on her own when working with cross validation. It harnesses functions from a myriad of other packages, and tailors their powers to perform CV and output the metrics Lizzie often needs for her work.

Cross validation is used to determine the effectiveness of a particular modeling method. In k-fold cross validation, rather than a single simple train-test split, we harness the power of the entire dataset to determine how the method performs on unseen data by iteratively applying the method to different subsets of the data and testing it on the unseen subset. The metrics calculated from k-fold cross validation are averaged across the k-folds and assumed to apply to the modeling method itself. Thus, once the predictive power of the method has been established through k-fold cross validation, typically we re-fit the final model using the method on the full dataset. There is some controversy over this refitting step as sometimes the size of a dataset can change predictive power, but this is the general practice.

Typically, data scientists are interested in calculating performance metrics on the out of fold predictions from cross validation and using them to determine model predictive power. However, Lizzie has found there is often more to consider than metrics such as the AIC, BIC, RMSE, R^2, etc. Particularly in the case of linear models, we can evaluate the stability of linear coefficients across models created from different segments of data. Further, we can consider how often models overfit the data across folds, or how often a certain coefficient was statistically significant. That's where this package comes into play. This package automates coefficient comparison across folds, and gives the user easy-to-work-with fields for folds and out of fold predictions, which allow the user to calculate their own creative metrics if they so desire.

## Installation

You can install the development version of CVComplete like so:

``` {r}
#for others:
#run devtools::install_github("easoneli176/CVComplete") to install package

#on my personal machine
library(devtools)
library(CVComplete)
```

## Example

This is a basic example which shows you how to solve a common problem:

```{r example}
library(CVComplete)

mock_data <- data.frame(preds = rep(1:4,10),target=rep(1:5,8),strat=rep(1:2,20))
#new data set:
newdat<-fold_field(mock_data,3,"strat")
#check that stratification worked:
table(newdat$fold,newdat$strat)

#Notes on package creation for Lizzie that probably belong in a template:
#when creating a package, use roxygen2::roxygenise() to create documentation after writing the source code .R file
#ctrl+shift+B rebuilds package after editing
#to connect existing package to github, make sure project has version control, commit changes, then run in the terminal:
#git remote add origin git@github.com:easoneli176/reponame.git (can copy this link from repo)
#git branch -M main
#git push -u origin main

#if packages need uploading:
#manually find the package in windows, they're stored here: C:\Users\eason\AppData\Local\R\win-library\4.2
#move the package out of that folder and keep it somewhere in case stuff breaks
#then install the newest version. If issues, can go to cran website and get zip file to move into folder
#The code `devtools::build_readme()` is handy for keeping README.md up to date. You could also use GitHub Actions to re-render `README.Rmd` every time you push. An example workflow can be found here: <https://github.com/r-lib/actions/tree/v1/examples>.
#This seems to fail without changing the output type to html, so trying that
```
